{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pj2Wn0R6CQv"
      },
      "source": [
        "1.1 Import necessary Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OXk_5Sk2RUk",
        "outputId": "38ba3fca-16c8-4e86-c792-16ddfbba0ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting brainiak\n",
            "  Downloading brainiak-0.11.tar.gz (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from brainiak) (0.29.36)\n",
            "Collecting mpi4py>=3 (from brainiak)\n",
            "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nitime (from brainiak)\n",
            "  Downloading nitime-0.10.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.17.* in /usr/local/lib/python3.10/dist-packages (from brainiak) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn[alldeps]>=0.18 in /usr/local/lib/python3.10/dist-packages (from brainiak) (1.2.2)\n",
            "Requirement already satisfied: scipy!=1.0.0 in /usr/local/lib/python3.10/dist-packages (from brainiak) (1.10.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from brainiak) (0.13.5)\n",
            "Collecting pymanopt (from brainiak)\n",
            "  Downloading pymanopt-2.1.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting theano>=1.0.4 (from brainiak)\n",
            "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=1.7 (from brainiak)\n",
            "  Using cached pybind11-2.11.0-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from brainiak) (5.9.5)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from brainiak) (3.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from brainiak) (1.3.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from brainiak) (0.40.0)\n",
            "Collecting pydicom (from brainiak)\n",
            "  Downloading pydicom-2.4.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: scikit-learn 1.2.2 does not provide the extra 'alldeps'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn[alldeps]>=0.18->brainiak) (3.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from theano>=1.0.4->brainiak) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nitime->brainiak) (3.7.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from nitime->brainiak) (3.1)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from statsmodels->brainiak) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->brainiak) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->brainiak) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->statsmodels->brainiak) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->statsmodels->brainiak) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nitime->brainiak) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nitime->brainiak) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nitime->brainiak) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nitime->brainiak) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nitime->brainiak) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nitime->brainiak) (3.1.0)\n",
            "Building wheels for collected packages: brainiak, mpi4py, theano\n",
            "  Building wheel for brainiak (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for brainiak: filename=brainiak-0.11-cp310-cp310-linux_x86_64.whl size=1138211 sha256=b518938e647557f19d8928fea304d9e25a2375cb36fc24d35c3b284f9d97e94e\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/8a/e7/33ffa8f6dd241864f592f7b1d20dc2db25bf7c38c2b5dc693d\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp310-cp310-linux_x86_64.whl size=3365662 sha256=9ca7d6e8d2740b001941c783db1967aecf94c26781c27ed7b0cc04c99868858a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/1b/b5/97ec4cfccdde26e0f3590ad6e09a5242d508dff09704ef86c1\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for theano: filename=Theano-1.0.5-py3-none-any.whl size=2668109 sha256=3ae34f35c33c911a369be3954a3a09ea60e412b3aedbc24cbf53365b51e02ef2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/e6/7d/2267d21a99e4ab8276f976f293b4ff23f50c9d809f4a216ebb\n",
            "Successfully built brainiak mpi4py theano\n",
            "Installing collected packages: pydicom, pybind11, mpi4py, theano, pymanopt, nitime, brainiak\n",
            "Successfully installed brainiak-0.11 mpi4py-3.1.4 nitime-0.10.1 pybind11-2.11.0 pydicom-2.4.1 pymanopt-2.1.1 theano-1.0.5\n"
          ]
        }
      ],
      "source": [
        "# download brainiak package\n",
        "! python3 -m pip install -U brainiak"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download nilearn\n",
        "!pip install -U nilearn"
      ],
      "metadata": {
        "id": "Q_O5_WjNNBEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3c282d-e3ea-42a1-e52f-0f7ba561e736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.1-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.3.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.3)\n",
            "Collecting nibabel>=3.2.0 (from nilearn)\n",
            "  Downloading nibabel-5.1.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (23.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Installing collected packages: nibabel, nilearn\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 3.0.2\n",
            "    Uninstalling nibabel-3.0.2:\n",
            "      Successfully uninstalled nibabel-3.0.2\n",
            "Successfully installed nibabel-5.1.0 nilearn-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download nibabel\n",
        "!pip install git+https://github.com/nipy/nibabel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S_Lzv2ZecyH",
        "outputId": "d0df6345-a4cb-4ffd-efcc-87b8abd4c63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/nipy/nibabel\n",
            "  Cloning https://github.com/nipy/nibabel to /tmp/pip-req-build-8lwzmzr_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/nipy/nibabel /tmp/pip-req-build-8lwzmzr_\n",
            "  Resolved https://github.com/nipy/nibabel to commit 6fe3e6b17857a5bba9e8fb31684d606dd458a6f1\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from nibabel==5.2.0.dev20+g6fe3e6b1) (1.22.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel==5.2.0.dev20+g6fe3e6b1) (23.1)\n",
            "Building wheels for collected packages: nibabel\n",
            "  Building wheel for nibabel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nibabel: filename=nibabel-5.2.0.dev20+g6fe3e6b1-py3-none-any.whl size=3289401 sha256=b7ca233bb952c243fea4caf1b72934d4e52b9a6bcb0e87209dfa382a8f31a1b4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kebrlc5z/wheels/5d/78/3b/248c139b24ead344e02b5b5d37a1b631e5e97be0770b7a153d\n",
            "Successfully built nibabel\n",
            "Installing collected packages: nibabel\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 5.1.0\n",
            "    Uninstalling nibabel-5.1.0:\n",
            "      Successfully uninstalled nibabel-5.1.0\n",
            "Successfully installed nibabel-5.2.0.dev20+g6fe3e6b1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-vx_0GZ2FIv"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "from pathlib import Path\n",
        "from brainiak.utils import fmrisim\n",
        "import nibabel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.ndimage as ndimage\n",
        "import scipy.spatial.distance as sp_distance\n",
        "import sklearn.manifold as manifold\n",
        "import scipy.stats as stats\n",
        "import sklearn.model_selection\n",
        "import sklearn.svm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khACwZN_6H0G"
      },
      "source": [
        "1.2 Load participant data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0br3mpetewI5",
        "outputId": "fd3bca43-6b88-4cc6-9a18-d589bbaa451c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLGUV6p8e9kT",
        "outputId": "d933a913-78fb-48a2-f9f7-4ae08557bc4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-89a5975caeeb>:3: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
            "\n",
            "* deprecated from version: 3.0\n",
            "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
            "  volume = nii.get_data()\n"
          ]
        }
      ],
      "source": [
        "home = '/content/drive/MyDrive'\n",
        "nii = nibabel.load(home + '/Corr_MVPA/Participant_01_rest_run01.nii')\n",
        "volume = nii.get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYPibM076MNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb639176-75b9-425f-ef74-630b81224510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Volume dimensions: (64, 64, 27, 294)\n",
            "TR duration: 1.50s\n",
            "(3.0, 3.0, 3.5000014, 1500.0)\n"
          ]
        }
      ],
      "source": [
        "dim = volume.shape  # What is the size of the volume\n",
        "dimsize = nii.header.get_zooms()  # Get voxel dimensions from the nifti header\n",
        "tr = dimsize[3]\n",
        "if tr > 100:  # If high then these values are likely in ms and so fix it\n",
        "    tr /= 1000\n",
        "print('Volume dimensions:', dim)\n",
        "print('TR duration: %0.2fs' % tr)\n",
        "print(dimsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnzVtxHqMEGB"
      },
      "source": [
        "1.3 Generate an activity template and a mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXDj8yhCfXcm"
      },
      "outputs": [],
      "source": [
        "mask, template = fmrisim.mask_brain(volume=volume,\n",
        "                                    mask_self=True,\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask.shape)\n",
        "print(template.shape)"
      ],
      "metadata": {
        "id": "M4qBEpKnMRUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6921fa-4f06-44f7-a028-c539a7584a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 64, 27)\n",
            "(64, 64, 27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL7tvV1_Q7kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c1936c-7760-445b-86c5-a4714f9d26ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/dataset/sub-01_T1w_class-GM_probtissue.nii.gz\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of drive/MyDrive/dataset/sub-01_T1w_class-GM_probtissue.nii.gz or\n",
            "        drive/MyDrive/dataset/sub-01_T1w_class-GM_probtissue.nii.gz.zip, and cannot find drive/MyDrive/dataset/sub-01_T1w_class-GM_probtissue.nii.gz.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "# unzip probability maps of gray matter, white matter, and CSF respectively\n",
        "!unzip drive/MyDrive/dataset/sub-01_T1w_class-GM_probtissue.nii.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgETZDLuQ8D9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066b669b-49a8-4aeb-e309-be6831088e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/dataset/sub-01_T1w_class-WM_probtissue.nii.gz\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of drive/MyDrive/dataset/sub-01_T1w_class-WM_probtissue.nii.gz or\n",
            "        drive/MyDrive/dataset/sub-01_T1w_class-WM_probtissue.nii.gz.zip, and cannot find drive/MyDrive/dataset/sub-01_T1w_class-WM_probtissue.nii.gz.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!unzip drive/MyDrive/dataset/sub-01_T1w_class-WM_probtissue.nii.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkzuXFcVQ--t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d651a018-6d07-4767-f8a8-ddf56c7d7d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/dataset/sub-01_T1w_class-CSF_probtissue.nii.gz\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of drive/MyDrive/dataset/sub-01_T1w_class-CSF_probtissue.nii.gz or\n",
            "        drive/MyDrive/dataset/sub-01_T1w_class-CSF_probtissue.nii.gz.zip, and cannot find drive/MyDrive/dataset/sub-01_T1w_class-CSF_probtissue.nii.gz.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!unzip drive/MyDrive/dataset/sub-01_T1w_class-CSF_probtissue.nii.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "# load functional data\n",
        "filepath_func = 'drive/MyDrive/dataset/sub-01_ses-localizer_task-objectcategories_run-1_bold_space-T1w_preproc.nii.gz'\n",
        "func = nib.load(filepath_func)"
      ],
      "metadata": {
        "id": "CoXJfnrZa7wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGG5eHd5SUY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee409c1-3e13-4f41-dc9c-514f57077082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(274, 384, 384)\n",
            "[[ 6.93359315e-01  4.50425558e-02  7.98166469e-02 -1.16284210e+02]\n",
            " [-3.23181227e-02  6.55866504e-01 -1.15482718e-01 -6.84598312e+01]\n",
            " [-9.06430706e-02  1.10701263e-01  6.51718795e-01 -1.61474640e+02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "# load gray matter\n",
        "filepath_gm = 'drive/MyDrive/dataset/sub-01_T1w_class-GM_probtissue.nii.gz'\n",
        "gm = nib.load(filepath_gm)\n",
        "print(gm.shape)\n",
        "print(gm.affine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCjo8oHXSUiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "0628b2b5-b25d-4704-ac5b-d6caca2aa068"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4030639811c1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import nibabel.affines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnibabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrescale_affine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnibabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnibp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# resize gray matter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfunc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'rescale_affine' from 'nibabel.affines' (/usr/local/lib/python3.10/dist-packages/nibabel/affines.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#import nibabel.affines\n",
        "from nibabel.affines import rescale_affine\n",
        "import nibabel.processing as nibp\n",
        "# resize gray matter\n",
        "func0 = func.slicer[:,:,:,0]\n",
        "print(func0)\n",
        "print(func0.shape)\n",
        "print(func0.affine)\n",
        "gm_funcSize=nibp.resample_from_to(gm, func0, order=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gm.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J7XUSyWdE4i",
        "outputId": "2356c2f7-04aa-47df-a173-7f328e66f15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(274, 384, 384)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUG1vdVKUTpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "50708114-a821-47bb-f4e7-719baec28c92"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-d68e3ca34e1f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# discretize gray matter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgm_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm_funcSize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgm_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgm_values\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gm_funcSize' is not defined"
          ]
        }
      ],
      "source": [
        "# discretize gray matter\n",
        "gm_values = gm_funcSize.get_fdata()\n",
        "gm_mask = (gm_values>0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv4vog2vUeCG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6a3416d4-af2b-4c08-d444-27ae2367cab4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-4ad1d64566c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gm_mask' is not defined"
          ]
        }
      ],
      "source": [
        "print(np.count_nonzero(gm_mask))\n",
        "print(gm_mask.size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gm_values.max())\n",
        "print(gm_values.min())"
      ],
      "metadata": {
        "id": "DRiVZsDyqvw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gm_mask_large = (gm_values>1)\n",
        "gm_mask_small = (gm_values<0.1)"
      ],
      "metadata": {
        "id": "CG25lQLCPg6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "# plt.imshow(gm_mask_large[:, :, 26], cmap=plt.cm.gray)\n",
        "plt.imshow(gm_mask_small[:, :, 26], cmap=plt.cm.gray, alpha=0.2)\n",
        "# plt.imshow(gm_mask[:, :, 26], cmap=plt.cm.gray, alpha=0.2)\n",
        "plt.axis('off')\n",
        "\n"
      ],
      "metadata": {
        "id": "BgHUG3HxeMSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNwqzrArU2Eb"
      },
      "outputs": [],
      "source": [
        "# load white matter and csf\n",
        "filepath_wm = 'drive/MyDrive/dataset/sub-01_T1w_class-WM_probtissue.nii.gz'\n",
        "wm = nib.load(filepath_wm)\n",
        "print(wm.shape)\n",
        "print(wm.affine)\n",
        "filepath_csf = 'drive/MyDrive/dataset/sub-01_T1w_class-CSF_probtissue.nii.gz'\n",
        "csf = nib.load(filepath_csf)\n",
        "print(wm.shape)\n",
        "print(csf.affine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBLrphEqVCyg"
      },
      "outputs": [],
      "source": [
        "# resize white matter and csf\n",
        "wm_funcSize=nibp.resample_from_to(wm, func0, order=1)\n",
        "csf_funcSize=nibp.resample_from_to(csf, func0, order=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nib.save(wm_funcSize, os.path.join('drive/MyDrive/dataset', 'wm_funcSize_2.nii.gz'))\n",
        "nib.save(csf_funcSize, os.path.join('drive/MyDrive/dataset', 'csf_funcSize_2.nii.gz'))"
      ],
      "metadata": {
        "id": "EciGK460Q-KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrh63zmPVMoK"
      },
      "outputs": [],
      "source": [
        "# discretize white matter and csf and make confounds mask\n",
        "wm_values = wm_funcSize.get_fdata()\n",
        "csf_values = csf_funcSize.get_fdata()\n",
        "confounds_values = wm_values+csf_values\n",
        "confounds_mask = (confounds_values>0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wm_values.max())\n",
        "print(wm_values.min())\n",
        "print(csf_values.max())\n",
        "print(csf_values.min())\n",
        "print(confounds_values.max())\n",
        "print(confounds_values.min())"
      ],
      "metadata": {
        "id": "GRD-VoMGHi80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_all = gm_values + wm_values + csf_values\n",
        "print(template_all.max())\n",
        "print(template_all.min())"
      ],
      "metadata": {
        "id": "FE_U77B0Nqot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "# plt.imshow(template_all[:, :, 26], cmap=plt.cm.gray)\n",
        "# plt.imshow(confounds_mask[:, :, 26], cmap=plt.cm.gray, alpha=0.5)\n",
        "plt.imshow(gm_mask[:, :, 26], cmap=plt.cm.gray, alpha=0.2)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "sUWZMiiNITWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtSCDN32VQYy"
      },
      "outputs": [],
      "source": [
        "print(np.count_nonzero(confounds_mask))\n",
        "print(confounds_mask.size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# brain_mask = gm_mask | confounds_mask\n",
        "diff = gm_mask & confounds_mask\n",
        "gm_mask_c = gm_mask ^ diff\n",
        "confounds_mask_c = confounds_mask ^diff\n",
        "brain_mask = gm_mask_c | confounds_mask_c\n",
        "print(np.count_nonzero(gm_mask_c))\n",
        "print(np.count_nonzero(confounds_mask_c))\n",
        "print(np.count_nonzero(brain_mask))"
      ],
      "metadata": {
        "id": "KFedxpNYLEec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "# plt.imshow(template_all[:, :, 26], cmap=plt.cm.gray)\n",
        "plt.imshow(confounds_mask_c[:, :, 26]*0.5, cmap='magma',vmin=0, vmax=1)\n",
        "plt.imshow(gm_mask_c[:, :, 26], cmap='magma',alpha=0.5,vmin=0, vmax=1)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "rf2pciXtNRZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLQkr9VZMk_7"
      },
      "source": [
        "1.4 Determine noise parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OywK1SwzMkaJ"
      },
      "outputs": [],
      "source": [
        "# Calculate the noise parameters from the data. Set it up to be matched.\n",
        "noise_dict = {'voxel_size': [dimsize[0],dimsize[1],dimsize[2]], 'matched': 1}\n",
        "noise_dict = fmrisim.calc_noise(volume=volume,\n",
        "                                mask=mask,\n",
        "                                template=template,\n",
        "                                noise_dict=noise_dict,\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxp5h3gOMUAW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "693cc319-01b8-4f59-d583-52c2b1dda8cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noise parameters of the data were estimated as follows:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5feaf1009929>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Noise parameters of the data were estimated as follows:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SNR: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'snr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SFNR: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sfnr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FWHM: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fwhm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'noise_dict' is not defined"
          ]
        }
      ],
      "source": [
        "print('Noise parameters of the data were estimated as follows:')\n",
        "print('SNR: ' + str(noise_dict['snr']))\n",
        "print('SFNR: ' + str(noise_dict['sfnr']))\n",
        "print('FWHM: ' + str(noise_dict['fwhm']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the different types of noise\n",
        "total_time = 500\n",
        "timepoints = list(range(0, total_time, int(tr)))\n",
        "\n",
        "drift = fmrisim._generate_noise_temporal_drift(total_time,\n",
        "                                               int(tr),\n",
        "                                               )\n",
        "\n",
        "mini_dim = np.array([2, 2, 2])\n",
        "autoreg = fmrisim._generate_noise_temporal_autoregression(timepoints,\n",
        "                                                          noise_dict,\n",
        "                                                          mini_dim,\n",
        "                                                          np.ones(mini_dim),\n",
        "                                                          )\n",
        "\n",
        "phys = fmrisim._generate_noise_temporal_phys(timepoints,\n",
        "                                            )\n",
        "\n",
        "stimfunc = np.zeros((int(total_time / tr), 1))\n",
        "stimfunc[np.random.randint(0, int(total_time / tr), 50)] = 1\n",
        "task = fmrisim._generate_noise_temporal_task(stimfunc,\n",
        "                                            )"
      ],
      "metadata": {
        "id": "locwEpn_MdYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the different noise types\n",
        "plt.figure(figsize=(30,7))\n",
        "plt.title('Noise types')\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(drift,color='k',linewidth=3)\n",
        "plt.axis('off')\n",
        "plt.xlabel('Drift')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(autoreg[0, 1, 0, :],color='k',linewidth=1.5)\n",
        "plt.axis('off')\n",
        "plt.xlabel('Autoregression')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(phys,color='k',linewidth=2)\n",
        "plt.axis('off')\n",
        "plt.xlabel('Physiological')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(task,color='k',linewidth=2)\n",
        "plt.axis('off')\n",
        "plt.xlabel('Task')\n",
        "plt.savefig('drive/MyDrive/dataset/fig3/noise.png',dpi=800)"
      ],
      "metadata": {
        "id": "FhYLcj4vMlO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RWBVaxqQo7O"
      },
      "source": [
        "**2. Generate noise**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "volume_c = (func.shape)\n",
        "print(volume_c)\n",
        "print(brain_mask.shape)\n",
        "dimsize_c = func.header.get_zooms()\n",
        "print(dimsize_c)"
      ],
      "metadata": {
        "id": "0U4jY9CTKKwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brain_mask_c = 1*brain_mask"
      ],
      "metadata": {
        "id": "uo851uPiMc8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(volume_c[0:3])\n",
        "print(dimsize_c[3])\n",
        "print(dim[3])"
      ],
      "metadata": {
        "id": "vCqDG5hmObnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SMUvl6kQvKC"
      },
      "outputs": [],
      "source": [
        "# Calculate the noise given the parameters\n",
        "noise = fmrisim.generate_noise(dimensions=volume_c[0:3],\n",
        "                               tr_duration=int(dimsize_c[3]),\n",
        "                               stimfunction_tr=[0] * volume_c[3],\n",
        "                               mask=brain_mask,\n",
        "                               template=template_all,\n",
        "                               noise_dict=noise_dict,\n",
        "                               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K535zMx9UOwh"
      },
      "outputs": [],
      "source": [
        "print(noise.shape)\n",
        "noise_reshaped = np.reshape(noise,[noise.shape[0]*noise.shape[1]*noise.shape[2],noise.shape[3]])\n",
        "volume_reshaped = np.reshape(volume,[volume.shape[0]*volume.shape[1]*volume.shape[2],volume.shape[3]])\n",
        "print(noise_reshaped.shape)\n",
        "print(volume_reshaped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu47r4zXTOlb"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(noise[37, 15, 25, :])\n",
        "idx = np.argwhere(np.all(noise_reshaped[..., :] == 0, axis=1))\n",
        "noise_c = np.delete(noise_reshaped, idx, axis=0)\n",
        "print(noise_c.shape)\n",
        "print(noise_c.std(axis=1).mean())\n",
        "print(noise_c.mean(axis=1).mean())\n",
        "print(np.divide(noise_c.std(axis=1),noise_c.mean(axis=1)).mean())\n",
        "\n",
        "idx2 = np.argwhere(np.all(volume_reshaped[..., :] == 0, axis=1))\n",
        "volume_c = np.delete(volume_reshaped, idx2, axis=0)\n",
        "print(volume_c.shape)\n",
        "print(volume_c.std(axis=1).mean())\n",
        "print(volume_c.mean(axis=1).mean())\n",
        "print(np.divide(volume_c.std(axis=1),volume_c.mean(axis=1)).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viOsRsc2UvWv"
      },
      "source": [
        "**3. Generate signal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeflVfITYezA"
      },
      "source": [
        "3.1 Specify which voxels in the brain contain signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0cFyRMaq7tf"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "x = [i for i in range(noise.shape[0])]\n",
        "y = [i for i in range(noise.shape[1])]\n",
        "z = [i for i in range(noise.shape[2])]\n",
        "coordinates = list(itertools.product(x,y,z))\n",
        "print(len(coordinates))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E4j-oiGT0UU"
      },
      "outputs": [],
      "source": [
        "# Create the region of activity where signal will appear\n",
        "coordinates = np.array(coordinates)  # Where in the brain is the signal\n",
        "feature_size = 1  # How big, in voxels, is the size of the ROI\n",
        "signal_volume = fmrisim.generate_signal(dimensions=noise.shape[0:3],\n",
        "                                        feature_type=['cube'],\n",
        "                                        feature_coordinates=coordinates,\n",
        "                                        feature_size=[feature_size],\n",
        "                                        signal_magnitude=[1],\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyQwZEZcr4Xn"
      },
      "outputs": [],
      "source": [
        "print(signal_volume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rENNYWQC_hfS"
      },
      "outputs": [],
      "source": [
        "signal_volume_c = np.copy(signal_volume)\n",
        "signal_volume_c[:,:,0] = 1.0\n",
        "signal_volume_c[:,0,:] = 1.0\n",
        "signal_volume_c[0,:,:] = 1.0\n",
        "print(signal_volume_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qwRdXQkYbiX"
      },
      "source": [
        "3.2 Characterize signal for voxels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDu2B3QoYcF8"
      },
      "outputs": [],
      "source": [
        "# Create a pattern for each voxel in our signal ROI\n",
        "# voxels = feature_size ** 3\n",
        "voxels = np.count_nonzero(signal_volume_c)\n",
        "print(voxels)\n",
        "\n",
        "# Pull the conical voxel activity from a uniform distribution\n",
        "pattern_A = np.random.rand(voxels).reshape((voxels, 1))\n",
        "pattern_B = np.random.rand(voxels).reshape((voxels, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWCvjyloYuUl"
      },
      "outputs": [],
      "source": [
        "print(pattern_B.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te4AoccSYjZh"
      },
      "outputs": [],
      "source": [
        "# Plot pattern of activity for each condition\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(pattern_A)\n",
        "plt.ylabel('Voxels')\n",
        "plt.tick_params(which='both', left='off', labelleft='off', bottom='off', labelbottom='off')\n",
        "plt.xlabel('Condition A')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(pattern_B)\n",
        "plt.tick_params(which='both', left='off', labelleft='off', bottom='off', labelbottom='off')\n",
        "plt.xlabel('Condition B')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buXVthtqY6uQ"
      },
      "source": [
        "3.3 Generate event time course"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLfPrFDYZOGy"
      },
      "outputs": [],
      "source": [
        "# Set up stimulus event time course parameters\n",
        "event_duration = 2  # How long is each event\n",
        "isi = 7  # What is the time between each event\n",
        "burn_in = 1  # How long before the first event\n",
        "\n",
        "total_time = int(noise.shape[3] * dimsize_c[3]) + burn_in  # How long is the total event time course\n",
        "events = int((total_time - ((event_duration + isi) * 2))  / ((event_duration + isi) * 2)) * 2  # How many events are there?\n",
        "onsets_all = np.linspace(burn_in, events * (event_duration + isi), events)  # Space the events out\n",
        "np.random.shuffle(onsets_all)  # Shuffle their order\n",
        "onsets_A = onsets_all[:int(events / 2)]  # Assign the first half of shuffled events to condition A\n",
        "onsets_B = onsets_all[int(events / 2):]  # Assign the second half of shuffled events to condition B\n",
        "temporal_res = 10.0 # How many timepoints per second of the stim function are to be generated?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsUcH5oNZU8z"
      },
      "outputs": [],
      "source": [
        "print(total_time)\n",
        "print(events)\n",
        "print(onsets_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgmZdG0nZ2tQ"
      },
      "outputs": [],
      "source": [
        "# Create a time course of events\n",
        "stimfunc_A = fmrisim.generate_stimfunction(onsets=onsets_A,\n",
        "                                           event_durations=[event_duration],\n",
        "                                           total_time=total_time,\n",
        "                                           temporal_resolution=temporal_res,\n",
        "                                           )\n",
        "\n",
        "stimfunc_B = fmrisim.generate_stimfunction(onsets=onsets_B,\n",
        "                                           event_durations=[event_duration],\n",
        "                                           total_time=total_time,\n",
        "                                           temporal_resolution=temporal_res,\n",
        "                                           )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyfEVVCLZ5m3"
      },
      "source": [
        "3.4 Export stimulus time course for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2988VnStZ30R"
      },
      "outputs": [],
      "source": [
        "fmrisim.export_epoch_file(stimfunction=[np.hstack((stimfunc_A, stimfunc_B))],\n",
        "                          filename=home + '/epoch_file.npy',\n",
        "                          tr_duration=tr,\n",
        "                          temporal_resolution=temporal_res,\n",
        "                          )\n",
        "\n",
        "fmrisim.export_3_column(stimfunction=stimfunc_A,\n",
        "                        filename=home + '/Condition_A.txt',\n",
        "                        temporal_resolution=temporal_res,\n",
        "                        )\n",
        "\n",
        "fmrisim.export_3_column(stimfunction=stimfunc_B,\n",
        "                        filename=home + '/Condition_B.txt',\n",
        "                        temporal_resolution=temporal_res,\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr0q3loyaMw_"
      },
      "source": [
        "3.5 Estimate the voxel weight for each event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_E-mOTjaNq2"
      },
      "outputs": [],
      "source": [
        "# Multiply each pattern by each voxel time course\n",
        "weights_A = np.matlib.repmat(stimfunc_A, 1, voxels).transpose() * pattern_A\n",
        "weights_B = np.matlib.repmat(stimfunc_B, 1, voxels).transpose() * pattern_B\n",
        "\n",
        "# Sum these time courses together\n",
        "stimfunc_weighted = weights_A + weights_B\n",
        "stimfunc_weighted = stimfunc_weighted.transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPl4scBQaQs0"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(stimfunc_weighted[:, 0])\n",
        "plt.title('Example voxel response time course')\n",
        "plt.xlabel('Upsampled time course')\n",
        "plt.ylabel('Response size')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmLlvnfUaXBv"
      },
      "source": [
        "3.6 Convolve each voxel’s time course with the Hemodynamic Response Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahpnygTvaXpz"
      },
      "outputs": [],
      "source": [
        "signal_func = fmrisim.convolve_hrf(stimfunction=stimfunc_weighted,\n",
        "                                   tr_duration=dimsize_c[3],\n",
        "                                   temporal_resolution=temporal_res,\n",
        "                                   scale_function=1,\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qujIs6WYaZ-1"
      },
      "outputs": [],
      "source": [
        "# Prepare the data to be plotted\n",
        "response = signal_func[0:100,5] * 2\n",
        "downsample_A = stimfunc_A[0:int(100*temporal_res * dimsize_c[3]):int(temporal_res * dimsize_c[3]), 0]\n",
        "downsample_B = stimfunc_B[0:int(100*temporal_res * dimsize_c[3]):int(temporal_res * dimsize_c[3]), 0]\n",
        "\n",
        "# Display signal\n",
        "plt.figure()\n",
        "# plt.title('Example event time course and voxel response')\n",
        "EventA = plt.plot(downsample_A, 'r', label='Event_A')\n",
        "EventB = plt.plot(downsample_B, 'g', label='Event_B')\n",
        "# plt.subplot(2,1,2)\n",
        "Response = plt.plot(response, 'k', label='Response')\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.yticks([],'')\n",
        "plt.xlabel('nth TR')\n",
        "plt.savefig('drive/MyDrive/dataset/fig3/event.png',dpi=800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD7_OK1gad3r"
      },
      "source": [
        "3.7 Establish signal magnitude\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mskQNdvUacWg"
      },
      "outputs": [],
      "source": [
        "# Specify the parameters for signal\n",
        "signal_method = 'CNR_Amp/Noise-SD'\n",
        "signal_magnitude = [1]\n",
        "\n",
        "# Where in the brain are there stimulus evoked voxels\n",
        "signal_idxs = np.where(signal_volume_c == 1)\n",
        "print(signal_idxs[0].shape)\n",
        "\n",
        "# Pull out the voxels corresponding to the noise volume\n",
        "noise_func = noise[signal_idxs[0], signal_idxs[1], signal_idxs[2], :].T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96172liFBir9"
      },
      "outputs": [],
      "source": [
        "print(signal_func.shape)\n",
        "print(noise_func.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHzlT4o-ahh4"
      },
      "outputs": [],
      "source": [
        "# Compute the signal appropriate scaled\n",
        "signal_func_scaled = fmrisim.compute_signal_change(signal_func,\n",
        "                                                  noise_func,\n",
        "                                                  noise_dict,\n",
        "                                                  magnitude=signal_magnitude,\n",
        "                                                  method=signal_method,\n",
        "                                                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mMowb6X8_Vi"
      },
      "outputs": [],
      "source": [
        "print(signal_func_scaled.shape)\n",
        "print(noise_func.shape)\n",
        "\n",
        "idx3 = np.argwhere(np.all(signal_func_scaled[..., :] == 0, axis=0))\n",
        "signal_c = np.delete(signal_func_scaled, idx, axis=1)\n",
        "print(signal_c.shape)\n",
        "print(signal_c.std(axis=0).mean())\n",
        "print(signal_c.mean(axis=0).mean())\n",
        "print(np.divide(signal_c.std(axis=0),signal_c.mean(axis=0)).mean())\n",
        "\n",
        "idx4 = np.argwhere(np.all(noise_func[..., :] == 0, axis=0))\n",
        "noise_c2 = np.delete(noise_func, idx, axis=1)\n",
        "print(noise_c2.shape)\n",
        "print(noise_c2.std(axis=0).mean())\n",
        "print(noise_c2.mean(axis=0).mean())\n",
        "print(np.divide(noise_c2.std(axis=0),noise_c2.mean(axis=0)).mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signal_mean = (1*signal_c).mean(axis=0)\n",
        "noise_std = noise_c2.std(axis=0)\n",
        "tSNR = np.divide(signal_mean,noise_std)\n",
        "print('(tSNR)Expected to around 10 (0.03-161.2): ',tSNR.mean())\n",
        "signal_amp = (1*signal_c).max(axis=0)-(1*signal_c).mean(axis=0)\n",
        "CNR_amp = np.divide(signal_amp,noise_std)\n",
        "print('(CNR_amp)Expected to around 37 (8.17-95.73): ',CNR_amp.mean())\n",
        "signal_std = (1*signal_c).std(axis=0)\n",
        "CNR_std = np.divide(signal_std,noise_std)\n",
        "print('(CNR_std)Expected to around 0.029 (0.01-0.79): ',CNR_std.mean())"
      ],
      "metadata": {
        "id": "xFaVqKxD_dFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smaUh11gBu9v"
      },
      "outputs": [],
      "source": [
        "signal_mean = (3*signal_c).mean(axis=0)\n",
        "noise_std = noise_c2.std(axis=0)\n",
        "tSNR = np.divide(signal_mean,noise_std)\n",
        "print('(tSNR)Expected to around 10 (0.03-161.2): ',tSNR.mean())\n",
        "signal_amp = (3*signal_c).max(axis=0)-(3*signal_c).min(axis=0)\n",
        "CNR_amp = np.divide(signal_amp,noise_std)\n",
        "print('(CNR_amp)Expected to around 37 (8.17-95.73): ',CNR_amp.mean())\n",
        "signal_std = (3*signal_c).std(axis=0)\n",
        "CNR_std = np.divide(signal_std,noise_std)\n",
        "print('(CNR_std)Expected to around 0.029 (0.01-0.79): ',CNR_std.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2KGAnA4aksW"
      },
      "source": [
        " 3.8 Multiply the convolved response with the signal voxels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuKvnWrqalX9"
      },
      "outputs": [],
      "source": [
        "signal = fmrisim.apply_signal(signal_func_scaled,\n",
        "                              signal_volume_c,\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pd9WiRY_JMu"
      },
      "outputs": [],
      "source": [
        "print(signal.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvx4_JYxr26a"
      },
      "outputs": [],
      "source": [
        "import random as rd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdkxiKDgpNu6"
      },
      "outputs": [],
      "source": [
        "print(gm_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLc015e3bt34"
      },
      "outputs": [],
      "source": [
        "signal_reshaped = np.reshape(signal,[signal.shape[0]*signal.shape[1]*signal.shape[2],signal.shape[3]])\n",
        "noise_reshaped = np.reshape(noise,[noise.shape[0]*noise.shape[1]*noise.shape[2],noise.shape[3]])\n",
        "gm_reshaped = np.reshape(gm_mask_c,-1)\n",
        "ground_truth_list = signal_reshaped[gm_reshaped,:]\n",
        "print(ground_truth_list.mean())\n",
        "print(ground_truth_list.mean(axis=1).std())\n",
        "print(ground_truth_list.std(axis=1).mean())\n",
        "observation_list = ground_truth_list + noise_reshaped[gm_reshaped,:]\n",
        "print(ground_truth_list.shape)\n",
        "print(observation_list.shape)\n",
        "confounds_mask_reshaped = np.reshape(confounds_mask_c,-1)\n",
        "noise_list = noise_reshaped[confounds_mask_reshaped,:]\n",
        "print(noise_list.shape)\n",
        "print(noise_list.mean())\n",
        "print(noise_list.mean(axis=1).std())\n",
        "print(noise_list.std(axis=1).mean())\n",
        "print(observation_list.mean())\n",
        "print(observation_list.mean(axis=1).std())\n",
        "print(observation_list.std(axis=1).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xBPUzbaoNzD"
      },
      "source": [
        "# Preproces of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvJjm1nptq2a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nibabel.processing as nibp\n",
        "import os\n",
        "import nibabel as nib\n",
        "import math\n",
        "import random\n",
        "import sklearn.preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import random as rd\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6aIuyb_odCH"
      },
      "outputs": [],
      "source": [
        "class Scaler():\n",
        "    def __init__(self,inputs):\n",
        "        self.data = inputs\n",
        "        self.mean = np.mean(inputs,axis=1)\n",
        "        self.std = np.std(inputs, axis=1)\n",
        "        self.vox, self.time = inputs.shape\n",
        "    def transform(self,inputs):\n",
        "        self.mean = np.reshape(self.mean,(self.vox,1))\n",
        "        self.m_large = np.repeat(self.mean,self.time,axis=1)\n",
        "        self.std = np.reshape(self.std,(self.vox,1))\n",
        "        self.s_large = np.repeat(self.std,self.time,axis=1)\n",
        "        return np.divide(inputs-self.m_large,self.s_large)\n",
        "    def inverse_transform(self,outputs):\n",
        "        return np.multiply(outputs,self.s_large)+self.m_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxl9GBFVomi2"
      },
      "outputs": [],
      "source": [
        "obs_scale = Scaler(observation_list)\n",
        "obs_list = obs_scale.transform(observation_list)\n",
        "noi_scale = Scaler(noise_list)\n",
        "noi_list = noi_scale.transform(noise_list)\n",
        "gt_scale = Scaler(ground_truth_list)\n",
        "gt_list = gt_scale.transform(ground_truth_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYpxkCEqouVQ"
      },
      "outputs": [],
      "source": [
        "x = [i for i in range(noise_list.shape[1])]\n",
        "plt.figure(figsize = (16,16))\n",
        "plt.subplot(6,2,1)\n",
        "plt.plot(x,observation_list[900],'-')\n",
        "plt.title('signal + noise')\n",
        "plt.subplot(6,2,2)\n",
        "plt.plot(x,observation_list[24],'-')\n",
        "plt.title('signal + noise')\n",
        "plt.subplot(6,2,3)\n",
        "plt.plot(x,ground_truth_list[900],'-')\n",
        "plt.title('ground truth of signal')\n",
        "plt.subplot(6,2,4)\n",
        "plt.plot(x,ground_truth_list[24],'-')\n",
        "plt.title('ground truth of signal')\n",
        "plt.subplot(6,2,5)\n",
        "plt.plot(x,noise_list[900],'-')\n",
        "plt.title('noise, not the noise added to the signal')\n",
        "plt.subplot(6,2,6)\n",
        "plt.plot(x,noise_list[24],'-')\n",
        "plt.title('noise, not the noise added to the signal')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('drive/MyDrive/dataset/obs_list.csv',obs_list,delimiter=',')\n",
        "np.savetxt('drive/MyDrive/dataset/gt_list.csv',gt_list,delimiter=',')\n",
        "np.savetxt('drive/MyDrive/dataset/noi_list.csv',noi_list,delimiter=',')"
      ],
      "metadata": {
        "id": "_d_jLRLVpp9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}